{
  "dataset_revision": "a76d16fae880597b9c73047b50159220a441cb54",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "2.7.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.932285,
            "f1": 0.929584,
            "f1_weighted": 0.932472,
            "precision": 0.930473,
            "precision_weighted": 0.935972,
            "recall": 0.932083,
            "recall_weighted": 0.932285,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.933653,
            "f1": 0.933033,
            "f1_weighted": 0.933438,
            "precision": 0.933302,
            "precision_weighted": 0.934365,
            "recall": 0.934099,
            "recall_weighted": 0.933653,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.927041,
            "f1": 0.923983,
            "f1_weighted": 0.926873,
            "precision": 0.923981,
            "precision_weighted": 0.928893,
            "recall": 0.926181,
            "recall_weighted": 0.927041,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.933881,
            "f1": 0.931165,
            "f1_weighted": 0.934475,
            "precision": 0.931685,
            "precision_weighted": 0.939524,
            "recall": 0.934927,
            "recall_weighted": 0.933881,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.932969,
            "f1": 0.92897,
            "f1_weighted": 0.933026,
            "precision": 0.927884,
            "precision_weighted": 0.933965,
            "recall": 0.931013,
            "recall_weighted": 0.932969,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.932969,
            "f1": 0.929979,
            "f1_weighted": 0.932039,
            "precision": 0.933033,
            "precision_weighted": 0.934542,
            "recall": 0.930619,
            "recall_weighted": 0.932969,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.930233,
            "f1": 0.926072,
            "f1_weighted": 0.929667,
            "precision": 0.927043,
            "precision_weighted": 0.930999,
            "recall": 0.926748,
            "recall_weighted": 0.930233,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.932057,
            "f1": 0.9272,
            "f1_weighted": 0.932216,
            "precision": 0.924794,
            "precision_weighted": 0.933647,
            "recall": 0.931383,
            "recall_weighted": 0.932057,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.936389,
            "f1": 0.935628,
            "f1_weighted": 0.936397,
            "precision": 0.933819,
            "precision_weighted": 0.938553,
            "recall": 0.939457,
            "recall_weighted": 0.936389,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.937301,
            "f1": 0.932083,
            "f1_weighted": 0.937402,
            "precision": 0.932188,
            "precision_weighted": 0.937879,
            "recall": 0.932426,
            "recall_weighted": 0.937301,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.932877,
        "f1": 0.92977,
        "f1_weighted": 0.9328,
        "precision": 0.92982,
        "precision_weighted": 0.934834,
        "recall": 0.931894,
        "recall_weighted": 0.932877,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.932877,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 38.46874213218689,
  "kg_co2_emissions": null
}
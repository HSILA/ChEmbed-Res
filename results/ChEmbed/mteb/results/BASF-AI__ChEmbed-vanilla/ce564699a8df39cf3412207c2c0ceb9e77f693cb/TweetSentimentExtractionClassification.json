{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.7.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.611771,
            "f1": 0.612004,
            "f1_weighted": 0.599629,
            "precision": 0.61311,
            "precision_weighted": 0.613711,
            "recall": 0.634938,
            "recall_weighted": 0.611771,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.599604,
            "f1": 0.599635,
            "f1_weighted": 0.587407,
            "precision": 0.600751,
            "precision_weighted": 0.602515,
            "recall": 0.623888,
            "recall_weighted": 0.599604,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.626486,
            "f1": 0.628195,
            "f1_weighted": 0.619613,
            "precision": 0.625388,
            "precision_weighted": 0.625719,
            "recall": 0.643002,
            "recall_weighted": 0.626486,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.613469,
            "f1": 0.616957,
            "f1_weighted": 0.608036,
            "precision": 0.620772,
            "precision_weighted": 0.621931,
            "recall": 0.633063,
            "recall_weighted": 0.613469,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.635257,
            "f1": 0.638483,
            "f1_weighted": 0.631069,
            "precision": 0.635684,
            "precision_weighted": 0.636734,
            "recall": 0.650753,
            "recall_weighted": 0.635257,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.628749,
            "f1": 0.63097,
            "f1_weighted": 0.62179,
            "precision": 0.630616,
            "precision_weighted": 0.632978,
            "recall": 0.648705,
            "recall_weighted": 0.628749,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.573005,
            "f1": 0.570938,
            "f1_weighted": 0.56192,
            "precision": 0.568548,
            "precision_weighted": 0.566207,
            "recall": 0.58807,
            "recall_weighted": 0.573005,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.625071,
            "f1": 0.626684,
            "f1_weighted": 0.617798,
            "precision": 0.624355,
            "precision_weighted": 0.625609,
            "recall": 0.642974,
            "recall_weighted": 0.625071,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.583192,
            "f1": 0.58156,
            "f1_weighted": 0.567476,
            "precision": 0.583626,
            "precision_weighted": 0.584682,
            "recall": 0.610198,
            "recall_weighted": 0.583192,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.603282,
            "f1": 0.608041,
            "f1_weighted": 0.598211,
            "precision": 0.603561,
            "precision_weighted": 0.600973,
            "recall": 0.620228,
            "recall_weighted": 0.603282,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.609989,
        "f1": 0.611347,
        "f1_weighted": 0.601295,
        "precision": 0.610641,
        "precision_weighted": 0.611106,
        "recall": 0.629582,
        "recall_weighted": 0.609989,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.609989,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 37.44486093521118,
  "kg_co2_emissions": null
}
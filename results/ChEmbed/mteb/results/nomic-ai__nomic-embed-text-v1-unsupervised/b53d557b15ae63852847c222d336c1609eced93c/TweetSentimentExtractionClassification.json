{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.7.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.567629,
            "f1": 0.571007,
            "f1_weighted": 0.562075,
            "precision": 0.567435,
            "precision_weighted": 0.566357,
            "recall": 0.583758,
            "recall_weighted": 0.567629,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.555178,
            "f1": 0.558495,
            "f1_weighted": 0.546214,
            "precision": 0.554206,
            "precision_weighted": 0.551405,
            "recall": 0.576085,
            "recall_weighted": 0.555178,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.548387,
            "f1": 0.552509,
            "f1_weighted": 0.543188,
            "precision": 0.547869,
            "precision_weighted": 0.545032,
            "recall": 0.563774,
            "recall_weighted": 0.548387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.540464,
            "f1": 0.54256,
            "f1_weighted": 0.534733,
            "precision": 0.550221,
            "precision_weighted": 0.549665,
            "recall": 0.557371,
            "recall_weighted": 0.540464,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.57442,
            "f1": 0.578163,
            "f1_weighted": 0.570246,
            "precision": 0.576109,
            "precision_weighted": 0.575133,
            "recall": 0.589587,
            "recall_weighted": 0.57442,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.563667,
            "f1": 0.567725,
            "f1_weighted": 0.558596,
            "precision": 0.565115,
            "precision_weighted": 0.563158,
            "recall": 0.580253,
            "recall_weighted": 0.563667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.518959,
            "f1": 0.520757,
            "f1_weighted": 0.510986,
            "precision": 0.516077,
            "precision_weighted": 0.513774,
            "recall": 0.535457,
            "recall_weighted": 0.518959,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.562818,
            "f1": 0.567212,
            "f1_weighted": 0.55817,
            "precision": 0.562755,
            "precision_weighted": 0.560326,
            "recall": 0.578245,
            "recall_weighted": 0.562818,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.539332,
            "f1": 0.541318,
            "f1_weighted": 0.529314,
            "precision": 0.537882,
            "precision_weighted": 0.536104,
            "recall": 0.560656,
            "recall_weighted": 0.539332,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.552066,
            "f1": 0.556761,
            "f1_weighted": 0.549316,
            "precision": 0.552823,
            "precision_weighted": 0.550037,
            "recall": 0.564102,
            "recall_weighted": 0.552066,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.552292,
        "f1": 0.555651,
        "f1_weighted": 0.546284,
        "precision": 0.553049,
        "precision_weighted": 0.551099,
        "recall": 0.568929,
        "recall_weighted": 0.552292,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.552292,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 36.79751443862915,
  "kg_co2_emissions": null
}
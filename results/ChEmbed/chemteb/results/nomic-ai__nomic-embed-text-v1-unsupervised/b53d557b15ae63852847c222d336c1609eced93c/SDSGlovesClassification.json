{
  "dataset_revision": "c723236c5ec417d79512e6104aca9d2cd88168f6",
  "task_name": "SDSGlovesClassification",
  "mteb_version": "2.7.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.7915,
            "f1": 0.45109,
            "f1_weighted": 0.879899,
            "precision": 0.503533,
            "precision_weighted": 0.993522,
            "recall": 0.646335,
            "recall_weighted": 0.7915,
            "ap": 0.997168,
            "ap_weighted": 0.997168
          },
          {
            "accuracy": 0.8175,
            "f1": 0.462989,
            "f1_weighted": 0.89582,
            "precision": 0.505893,
            "precision_weighted": 0.994225,
            "recall": 0.721637,
            "recall_weighted": 0.8175,
            "ap": 0.99777,
            "ap_weighted": 0.99777
          },
          {
            "accuracy": 0.5835,
            "f1": 0.374002,
            "f1_weighted": 0.733245,
            "precision": 0.501706,
            "precision_weighted": 0.993459,
            "recall": 0.604167,
            "recall_weighted": 0.5835,
            "ap": 0.996831,
            "ap_weighted": 0.996831
          },
          {
            "accuracy": 0.7105,
            "f1": 0.423616,
            "f1_weighted": 0.827002,
            "precision": 0.503246,
            "precision_weighted": 0.993929,
            "recall": 0.667922,
            "recall_weighted": 0.7105,
            "ap": 0.997341,
            "ap_weighted": 0.997341
          },
          {
            "accuracy": 0.8485,
            "f1": 0.471794,
            "f1_weighted": 0.914295,
            "precision": 0.505422,
            "precision_weighted": 0.993705,
            "recall": 0.67495,
            "recall_weighted": 0.8485,
            "ap": 0.997396,
            "ap_weighted": 0.997396
          },
          {
            "accuracy": 0.71,
            "f1": 0.421809,
            "f1_weighted": 0.826746,
            "precision": 0.50204,
            "precision_weighted": 0.993222,
            "recall": 0.605422,
            "recall_weighted": 0.71,
            "ap": 0.996841,
            "ap_weighted": 0.996841
          },
          {
            "accuracy": 0.5505,
            "f1": 0.362056,
            "f1_weighted": 0.706005,
            "precision": 0.503411,
            "precision_weighted": 0.995121,
            "recall": 0.712098,
            "recall_weighted": 0.5505,
            "ap": 0.997695,
            "ap_weighted": 0.997695
          },
          {
            "accuracy": 0.3185,
            "f1": 0.24422,
            "f1_weighted": 0.479262,
            "precision": 0.499469,
            "precision_weighted": 0.991309,
            "recall": 0.471135,
            "recall_weighted": 0.3185,
            "ap": 0.99577,
            "ap_weighted": 0.99577
          },
          {
            "accuracy": 0.4385,
            "f1": 0.3093,
            "f1_weighted": 0.605638,
            "precision": 0.501516,
            "precision_weighted": 0.99374,
            "recall": 0.593624,
            "recall_weighted": 0.4385,
            "ap": 0.996747,
            "ap_weighted": 0.996747
          },
          {
            "accuracy": 0.6725,
            "f1": 0.412145,
            "f1_weighted": 0.800233,
            "precision": 0.504922,
            "precision_weighted": 0.995299,
            "recall": 0.773343,
            "recall_weighted": 0.6725,
            "ap": 0.998185,
            "ap_weighted": 0.998185
          }
        ],
        "accuracy": 0.64415,
        "f1": 0.393302,
        "f1_weighted": 0.766814,
        "precision": 0.503116,
        "precision_weighted": 0.993753,
        "recall": 0.647063,
        "recall_weighted": 0.64415,
        "ap": 0.997174,
        "ap_weighted": 0.997174,
        "main_score": 0.64415,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 58.85882306098938,
  "kg_co2_emissions": null
}
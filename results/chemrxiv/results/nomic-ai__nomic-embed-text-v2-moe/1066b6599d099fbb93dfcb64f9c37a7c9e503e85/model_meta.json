{"loader_kwargs": {"trust_remote_code": true, "model_prompts": {"Classification": "classification: ", "MultilabelClassification": "classification: ", "Clustering": "clustering: ", "PairClassification": "classification: ", "Reranking": "classification: ", "STS": "classification: ", "Summarization": "classification: ", "query": "search_query: ", "document": "search_document: "}, "model_kwargs": {"trust_remote_code": true, "dtype": "torch.bfloat16"}}, "name": "nomic-ai/nomic-embed-text-v2-moe", "revision": "1066b6599d099fbb93dfcb64f9c37a7c9e503e85", "release_date": "2025-02-07", "languages": ["eng-Latn", "spa-Latn", "fra-Latn", "deu-Latn", "ita-Latn", "por-Latn", "pol-Latn", "nld-Latn", "tur-Latn", "jpn-Jpan", "vie-Latn", "rus-Cyrl", "ind-Latn", "arb-Arab", "ces-Latn", "ron-Latn", "swe-Latn", "ell-Grek", "ukr-Cyrl", "zho-Hans", "hun-Latn", "dan-Latn", "nor-Latn", "hin-Deva", "fin-Latn", "bul-Cyrl", "kor-Hang", "slk-Latn", "tha-Thai", "heb-Hebr", "cat-Latn", "lit-Latn", "fas-Arab", "msa-Latn", "slv-Latn", "lav-Latn", "mar-Deva", "ben-Beng", "sqi-Latn", "cym-Latn", "bel-Cyrl", "mal-Mlym", "kan-Knda", "mkd-Cyrl", "urd-Arab", "fry-Latn", "fil-Latn", "tel-Telu", "eus-Latn", "swh-Latn", "som-Latn", "snd-Arab", "uzb-Latn", "cos-Latn", "hrv-Latn", "guj-Gujr", "hin-Latn", "ceb-Latn", "epo-Latn", "jav-Latn", "lat-Latn", "zul-Latn", "mon-Cyrl", "sin-Sinh", "ell-Latn", "gle-Latn", "kir-Cyrl", "tgk-Cyrl", "mya-Mymr", "khm-Khmr", "mlg-Latn", "pan-Guru", "rus-Latn", "sna-Latn", "zho-Latn", "hau-Latn", "heb-Latn", "hmn-Latn", "hat-Latn", "jpn-Latn", "sun-Latn", "bul-Latn", "gla-Latn", "nya-Latn", "pus-Arab", "kur-Latn", "hbs-Latn", "amh-Ethi", "ibo-Latn", "lao-Laoo", "mri-Latn", "nno-Latn", "smo-Latn", "yid-Hebr", "sot-Latn", "tgl-Latn", "xho-Latn", "yor-Latn"], "n_parameters": 475292928, "n_active_parameters_override": 141628032, "n_embedding_parameters": 192036864, "memory_usage_mb": 1813.0, "max_tokens": 512.0, "embed_dim": 768, "license": "apache-2.0", "open_weights": true, "public_training_code": "https://github.com/nomic-ai/contrastors/blob/613ddfd37309e538cceadb05b1e6423e7b09f603/src/contrastors/configs/train/contrastive_finetune_moe.yaml", "public_training_data": "https://github.com/nomic-ai/contrastors?tab=readme-ov-file#data-access", "framework": ["Sentence Transformers", "PyTorch", "safetensors"], "reference": "https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe", "similarity_fn_name": "cosine", "use_instructions": true, "training_datasets": null, "adapted_from": "nomic-ai/nomic-xlm-2048", "superseded_by": null, "modalities": ["text"], "model_type": ["dense"], "citation": "@misc{nussbaum2025trainingsparsemixtureexperts,\n      title={Training Sparse Mixture Of Experts Text Embedding Models},\n      author={Zach Nussbaum and Brandon Duderstadt},\n      year={2025},\n      eprint={2502.07972},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.07972},\n}", "contacts": null, "loader": "NomicWrapper", "is_cross_encoder": false}
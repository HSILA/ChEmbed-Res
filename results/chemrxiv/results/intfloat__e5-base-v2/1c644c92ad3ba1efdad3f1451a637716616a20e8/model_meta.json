{"loader_kwargs": {"model_prompts": {"query": "query: ", "document": "passage: "}, "trust_remote_code": true}, "name": "intfloat/e5-base-v2", "revision": "1c644c92ad3ba1efdad3f1451a637716616a20e8", "release_date": "2024-02-08", "languages": ["eng-Latn"], "n_parameters": 109000000, "memory_usage_mb": 418.0, "max_tokens": 512.0, "embed_dim": 768, "license": "mit", "open_weights": true, "public_training_code": null, "public_training_data": null, "framework": ["Sentence Transformers", "PyTorch", "ONNX", "safetensors"], "reference": "https://huggingface.co/intfloat/e5-base-v2", "similarity_fn_name": "cosine", "use_instructions": true, "training_datasets": ["NQHardNegatives", "mMARCO-NL", "NanoNQRetrieval", "MSMARCO", "MSMARCOHardNegatives", "NQ-NL", "NQ-PL", "MSMARCO-PL", "NanoMSMARCORetrieval", "NQ"], "adapted_from": "intfloat/e5-base", "superseded_by": null, "modalities": ["text"], "model_type": ["dense"], "citation": "\n@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n", "contacts": null, "loader": "sentence_transformers_loader", "is_cross_encoder": false}
{"loader_kwargs": {"model_prompts": {"query": "query: ", "document": "passage: "}, "trust_remote_code": true}, "name": "intfloat/e5-large", "revision": "4dc6d853a804b9c8886ede6dda8a073b7dc08a81", "release_date": "2022-12-26", "languages": ["eng-Latn"], "n_parameters": 335000000, "memory_usage_mb": 1278.0, "max_tokens": 512.0, "embed_dim": 1024, "license": "apache-2.0", "open_weights": true, "public_training_code": null, "public_training_data": null, "framework": ["Sentence Transformers", "PyTorch", "safetensors"], "reference": "https://huggingface.co/intfloat/e5-large", "similarity_fn_name": "cosine", "use_instructions": true, "training_datasets": ["NQHardNegatives", "mMARCO-NL", "NanoNQRetrieval", "MSMARCO", "MSMARCOHardNegatives", "NQ-NL", "NQ-PL", "MSMARCO-PL", "NanoMSMARCORetrieval", "NQ"], "adapted_from": "google-bert/bert-large-uncased-whole-word-masking", "superseded_by": "intfloat/e5-large-v2", "modalities": ["text"], "model_type": ["dense"], "citation": "\n@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n", "contacts": null, "loader": "sentence_transformers_loader", "is_cross_encoder": false}